{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rQ6G65n6OxV"
   },
   "source": [
    "# Create Embedding\n",
    "In this notebook, we'll connect to a Neo4j instance.  We'll load data based on a schema and compute graph embeddings.  The notebook exports that data to pandas and then writes them to Cloud Storage as CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MwTYwKk6OxX"
   },
   "source": [
    "## Using the Neo4j API\n",
    "Let's connect to our Neo4j deployment.  First off, install the Neo4j Graph Data Science package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15528,
     "status": "ok",
     "timestamp": 1681110797351,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "id": "FT0KaLYj6OxX",
    "outputId": "0298985b-0f45-4321-d94d-6da99305725c"
   },
   "outputs": [],
   "source": [
    "!pip install graphdatascience --quiet\n",
    "!pip install --quiet google-cloud-storage\n",
    "!pip install --quiet google.cloud.aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFokFbiL6OxY"
   },
   "source": [
    "Now, you're going to need the connection string and credentials from the deployment you created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1681109793559,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "gather": {
     "logged": 1669344803314
    },
    "id": "P41l_P4zzSqF"
   },
   "outputs": [],
   "source": [
    "# Edit these variables! \n",
    "DB_URL = '' #'neo4j+s://URL.databases.neo4j.io'\n",
    "DB_PASS = ''\n",
    "\n",
    "# You can leave this default\n",
    "DB_USER = 'neo4j'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create GDS connection object using the variables defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2309,
     "status": "ok",
     "timestamp": 1681109795865,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "gather": {
     "logged": 1669344806183
    },
    "id": "8lUkSvmozSqF"
   },
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "gds = GraphDataScience(DB_URL, auth=(DB_USER, DB_PASS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjzOZkVD8uLe",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Explore & Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are going to use is from a public available [Kaggle dataset](https://www.kaggle.com/datasets/rohitrox/healthcare-provider-fraud-detection-analysis).  These are healthcare expense claims with anonymised beneficiaries, claims and providers.  We've filtered the data and cleaned up the datasets. The cleaned data can be downloaded [here](https://storage.googleapis.com/neo4j-datasets/insurance-claim/data.csv)\n",
    "\n",
    "We will predict the potentially fraudulent providers based on the claims filed by them. We will use a GDS embedding algorithm to chart out Fraudulent patterns in the provider's claims to understand the future behaviour of providers.\n",
    "\n",
    "The dataset has\n",
    "\n",
    "- **Inpatient Data**: \n",
    "Contains claims filed for those patients who are admitted in the hospitals. It also provides additional details like their admission and discharge dates and admit and diagnosis code.\n",
    "- **Outpatient Data**\n",
    "- **Beneficiary Details Data**: \n",
    "Contains beneficiary KYC details like health conditions,regioregion they belong to etc. \n",
    "\n",
    "\n",
    "Before loading data into any Database, we usually have to come up with a schema and implement it on the Database. Graph Data Modelling is an important step with Neo4j and you define it based on the questions you would like to ask on the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, you can explore the data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Let\n",
    "raw_df = pd.read_csv('https://storage.googleapis.com/neo4j-datasets/insurance-claim/data.csv', \n",
    "                     index_col=False, dtype='unicode', parse_dates=['claim_start_dt', 'claim_end_dt', 'admission_dt', 'discharge_dt'])\n",
    "raw_df.deductible_amt_paid = raw_df.deductible_amt_paid.astype(float)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go with the following schema as our questions are more focussed around the relationships between claims, providers, physicians and the diagnoses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://storage.googleapis.com/neo4j-datasets/insurance-claim/img/schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILyN0cr18uLe",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create Constraints\n",
    "In order to ensure uniqueness of nodes, lets create some constraints. This will ensure that no duplicate nodes are created and speed up the CSV loading process, especially if we want to use `MERGE` statements - as MERGE statement creates new nodes only if they don't exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1669263867155
    },
    "id": "s00ivuUo8uLe",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "3d4af302-b77b-419c-f441-a0546bf02bf9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "node_labels = ['Beneficiary', 'County', 'State', 'Claim', 'DiagnosisGroup', 'AdmitDiagnosis', 'Physician', 'Provider', 'Diagnosis', 'Procedure']\n",
    "\n",
    "def to_snake_case(s: str) -> str:\n",
    "    return re.sub(r'(?<!^)(?=[A-Z])', '_', s).lower()\n",
    "\n",
    "for node_label in node_labels:\n",
    "    gds.run_cypher(f'CREATE CONSTRAINT {to_snake_case(node_label) + \"_node_key\"} IF NOT EXISTS FOR (n:{node_label}) REQUIRE n.uid IS NODE KEY;')\n",
    "\n",
    "# Unlike above nodes, Condition  will use a \"name\" property as a unique key.\n",
    "gds.run_cypher(f'CREATE CONSTRAINT condition_node_key IF NOT EXISTS FOR (n:Condition) REQUIRE n.name IS NODE KEY;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2xwss1p8uLf",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load Data\n",
    "\n",
    "Now, we're going to take data from the Google Cloud Storage bucket and import it into Neo4j.  There are a few different ways to do this.  We'll do with a naive LOAD CSV statements via the GDS Python API.  \n",
    "\n",
    "The Neo4j [Data Importer](https://data-importer.neo4j.io/) is another option.  It's a great graphical way to import data.  However, the LOAD CSV option we're using makes it really easy to pull directly from Cloud Storage, so is probably a better choice for what we need.\n",
    "\n",
    "Lets start creating the `Beneficiary`, `Claim`, `Provider`, `County` and `State` nodes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1669263891382
    },
    "id": "LOdBOBuI8uLf",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "e6190d56-2a46-4c99-d1ff-52afd2a33d3b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def chunks(xs, n=30_000):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]\n",
    "\n",
    "records = raw_df.to_dict('records')\n",
    "print('======  loading Beneficiary, Provider, Country, State, and Claim nodes  ======')\n",
    "\n",
    "cumulative_count = 0\n",
    "for recs in chunks(records):\n",
    "    gds.run_cypher(\n",
    "      \"\"\"\n",
    "        UNWIND $records AS row\n",
    "\n",
    "        MERGE (bene:Beneficiary {uid: row.bene_id})\n",
    "        ON CREATE SET\n",
    "            bene.dob = row.dob,\n",
    "            bene.gender = row.gender,\n",
    "            bene.race = row.race,\n",
    "            bene.ipAnnualReimbursementAmt = row.ip_annual_reimbursement_amt,\n",
    "            bene.opAnnualReimbursementAmt = row.op_annual_reimbursement_amt,\n",
    "            bene.ipAnnualDeductibleAmt = row.ip_annual_deductible_amt,\n",
    "            bene.opAnnualDeductibleAmt = row.op_annual_deductible_amt,\n",
    "            bene.partACovMonths = row.num_of_months_part_a_cov,\n",
    "            bene.partBCovMonths = row.num_of_months_part_b_Cov,\n",
    "            bene.dod = row.dod\n",
    "\n",
    "        MERGE (provider:Provider {uid: row.provider})\n",
    "\n",
    "        MERGE (county:County {uid: row.county})\n",
    "\n",
    "        MERGE (state:State {uid: row.state})\n",
    "\n",
    "        MERGE (claim:Claim {uid: row.claim_id})\n",
    "            SET claim.startDate = row.claim_start_dt,\n",
    "                claim.endDate = row.claim_end_dt,\n",
    "                claim.reimbursedAmt = row.claim_amt_reimbursed,\n",
    "                claim.isFraud = row.is_fraud,\n",
    "                claim.dischargeDate = row.discharge_dt,\n",
    "                claim.admitDate = row.admission_dt,\n",
    "                claim.deductibleAmtPaid = row.deductible_amt_paid\n",
    "      \"\"\"\n",
    "    , params={'records': recs})\n",
    "    cumulative_count += len(recs)\n",
    "    print(f'Loaded {cumulative_count:,} of {len(records):,} records')\n",
    "print(f'Loading Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the schema we agreed upon earlier, lets start to connect the ndoes we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = raw_df[['bene_id', 'claim_id', 'provider', 'county', 'state']].to_dict('records')\n",
    "\n",
    "result = gds.run_cypher(\n",
    "  \"\"\"\n",
    "    UNWIND $records AS row\n",
    "    MATCH (bene:Beneficiary {uid: row.bene_id})\n",
    "    MATCH (claim:Claim {uid: row.claim_id})\n",
    "    MATCH (provider:Provider {uid: row.provider})\n",
    "    MATCH (county:County {uid: row.county})\n",
    "    MATCH (state:State {uid: row.state})\n",
    "\n",
    "    MERGE (county)<-[:LOCATED_AT]-(bene)\n",
    "    MERGE (bene)-[:FILED_CLAIM]->(claim)-[:PROVIDED_BY]->(provider)\n",
    "    MERGE (state)<-[:PART_OF]-(county)\n",
    "  \"\"\"\n",
    "    , params={'records': records})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to create the `Physician` nodes and relate them to Claims and Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a reshaped physician dataframe to make loading more efficient\n",
    "physician_role_map = {'operating_physician':'OPERATED_BY', 'attending_physician':'ATTENDED_BY', 'other_physician':'ALSO_ATTENDED'}\n",
    "physician_dfs = []\n",
    "for col, role in physician_role_map.items():\n",
    "    temp_df = raw_df[['claim_id', 'provider', col]].rename(columns={col: 'physician_id'}).dropna()\n",
    "    temp_df['physician_role'] = role\n",
    "    physician_dfs.append(temp_df)\n",
    "physician_df = pd.concat(physician_dfs)\n",
    "physician_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Physician nodes\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "      UNWIND $physicianIds AS physicianId\n",
    "      MERGE (physician:Physician {uid: physicianId})\n",
    "    \"\"\"\n",
    "    , params={'physicianIds': physician_df.physician_id.drop_duplicates().tolist()})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Physician worked for provider relationship\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "      UNWIND $records AS row\n",
    "      MATCH (provider:Provider {uid: row.provider})\n",
    "      MATCH (physician:Physician {uid: row.physician_id})\n",
    "      MERGE (provider)<-[:WORKS_FOR]-(physician)\n",
    "    \"\"\"\n",
    "    , params={'records': physician_df[['provider','physician_id']].drop_duplicates().to_dict('records')})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Physician claim relationships\n",
    "for role in physician_role_map.values():\n",
    "    print(f'Loading {role} relationships...')\n",
    "    gds.run_cypher(\n",
    "        f\"\"\"\n",
    "          UNWIND $records AS row\n",
    "          MATCH (physician:Physician {{uid: row.physician_id}})\n",
    "          MATCH (claim:Claim {{uid: row.claim_id}})\n",
    "          MERGE (physician)<-[:{role}]-(claim)\n",
    "        \"\"\"\n",
    "        , params={'records': physician_df.loc[physician_df.physician_role == role, ['claim_id','physician_id']].to_dict('records')})\n",
    "print('Loading complete')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then create `AdmitDiagnosis` and `DiagnosisGroup` nodes and relate them to Claims and Providers as well"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#load admin diagnosis codes\n",
    "print(f'Loading AdmitDiagnosis nodes...')\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "      UNWIND $claim_admit_diagnosis_codes as claim_admit_diagnosis_code\n",
    "      MERGE (admitDiagnosis:AdmitDiagnosis {uid: claim_admit_diagnosis_code})\n",
    "    \"\"\"\n",
    "    , params={'claim_admit_diagnosis_codes': raw_df.claim_admit_diagnosis_code.dropna().drop_duplicates().to_list()})\n",
    "\n",
    "#load admin diagnosis relationships\n",
    "print(f'Loading HAS_ADMIT_DIAGNOSIS relationships...')\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "      UNWIND $records AS row\n",
    "      MATCH (claim:Claim {uid: row.claim_id})\n",
    "      MATCH (admitDiagnosis:AdmitDiagnosis {uid: row.claim_admit_diagnosis_code})\n",
    "      MERGE (claim)-[:HAS_ADMIT_DIAGNOSIS]->(admitDiagnosis)\n",
    "    \"\"\"\n",
    "    , params={'records': raw_df[['claim_id','claim_admit_diagnosis_code']].dropna().to_dict('records')})\n",
    "print('Loading complete')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#load admin diagnosis codes\n",
    "print(f'Loading DiagnosisGroup nodes...')\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "      UNWIND $diag_group_codes as diag_group_code\n",
    "      MERGE (diagnosisGroup:DiagnosisGroup {uid: diag_group_code})\n",
    "    \"\"\"\n",
    "    , params={'diag_group_codes': raw_df.diag_group_code.dropna().drop_duplicates().to_list()})\n",
    "\n",
    "#load admin diagnosis relationships\n",
    "print(f'Loading HAS_DIAGNOSIS_GROUP relationships...')\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "      UNWIND $records AS row\n",
    "      MATCH (claim:Claim {uid: row.claim_id})\n",
    "      MATCH (diagnosisGroup:DiagnosisGroup {uid: row.diag_group_code})\n",
    "      MERGE (claim)-[:HAS_DIAGNOSIS_GROUP]->(diagnosisGroup)\n",
    "    \"\"\"\n",
    "    , params={'records': raw_df[['claim_id','diag_group_code']].dropna().to_dict('records')})\n",
    "print('Loading complete')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the schema, Claims are related to procedures. Let deal with those nodes & relationships now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a reshaped procedure dataframe to make loading more efficient\n",
    "proc_cols = [col for col in raw_df.columns if 'claim_procedure_code' in col]\n",
    "proc_df = pd.wide_to_long(raw_df[proc_cols  + ['claim_id']], stubnames='claim_procedure_code', i='claim_id', j='proc', sep='_').dropna().reset_index()\n",
    "proc_df.claim_procedure_code = pd.to_numeric(proc_df.claim_procedure_code).astype('Int64')\n",
    "proc_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load procedure nodes\n",
    "print(f'Loading Procedure nodes...')\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "      UNWIND $claim_procedure_codes as claim_procedure_code\n",
    "      MERGE (proc:Procedure {uid: claim_procedure_code})\n",
    "    \"\"\"\n",
    "    , params={'claim_procedure_codes': proc_df.claim_procedure_code.drop_duplicates().to_list()})\n",
    "\n",
    "#load procedure - claim relationships\n",
    "print(f'Loading HAS_PROCEDURE relationships...')\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "      UNWIND $records AS row\n",
    "      MATCH (claim:Claim {uid: row.claim_id})\n",
    "      MATCH (proc:Procedure {uid: row.claim_procedure_code})\n",
    "      MERGE (claim)-[:HAS_PROCEDURE]->(proc)\n",
    "    \"\"\"\n",
    "    , params={'records': proc_df.to_dict('records')})\n",
    "print('Loading complete')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As per the schema, Claims also have diagnoses. Let deal with those nodes & relationships here too"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a reshaped claim diagnosis dataframe to make loading more efficient\n",
    "diag_cols = [col for col in raw_df.columns if 'claim_diag_code' in col]\n",
    "claim_diag_df = pd.wide_to_long(raw_df[diag_cols  + ['claim_id']], stubnames='claim_diag_code', i='claim_id', j='claim_number', sep='_').dropna().reset_index()\n",
    "claim_diag_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load diagnosis claim nodes\n",
    "print(f'Loading Diagnosis nodes...')\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "      UNWIND $claim_diag_code as claim_diag_code\n",
    "      MERGE (diag:Diagnosis {uid: claim_diag_code})\n",
    "    \"\"\"\n",
    "    , params={'claim_diag_code': claim_diag_df.claim_diag_code.drop_duplicates().to_list()})\n",
    "\n",
    "#load diagnosis - claim relationships\n",
    "print(f'Loading HAS_DIAGNOSIS relationships...')\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "      UNWIND $records AS row\n",
    "      MATCH (claim:Claim {uid: row.claim_id})\n",
    "      MATCH (diag:Diagnosis {uid: row.claim_diag_code})\n",
    "      MERGE (claim)-[:HAS_DIAGNOSIS]->(diag)\n",
    "    \"\"\"\n",
    "    , params={'records': claim_diag_df.to_dict('records')})\n",
    "print('Loading complete')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets connect our Benificiaries (or) patients with diseases they suffer from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a reshaped chronic condition dataframe to make loading more efficient\n",
    "cond_cols = [col for col in raw_df.columns if 'chronic_cond' in col]\n",
    "cond_df = (pd.wide_to_long(raw_df[cond_cols  + ['bene_id']].drop_duplicates(), stubnames='chronic_cond', i='bene_id', j='condition', sep='_', suffix='\\D+')\n",
    "           .reset_index().query('chronic_cond == \"1\"').drop(columns='chronic_cond'))\n",
    "cond_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load condition nodes\n",
    "print(f'Loading Condition nodes...')\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "      UNWIND $conditions as condition\n",
    "      MERGE (cond:Condition {name:condition})\n",
    "    \"\"\"\n",
    "    , params={'conditions': cond_df.condition.drop_duplicates().to_list()})\n",
    "\n",
    "#load condition - beneficiary relationships\n",
    "print(f'Loading HAS_CHRONIC relationships...')\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "      UNWIND $records AS row\n",
    "      MATCH (bene:Beneficiary {uid: row.bene_id})\n",
    "      MATCH (cond:Condition {name:row.condition})\n",
    "      MERGE (bene)-[:HAS_CHRONIC]->(cond)\n",
    "    \"\"\"\n",
    "    , params={'records': cond_df.to_dict('records')})\n",
    "print('Loading complete')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a breakdown of high-level counts by labels and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total node counts\n",
    "gds.run_cypher(\"\"\"\n",
    "    CALL apoc.meta.stats()\n",
    "    YIELD labels\n",
    "    UNWIND keys(labels) AS nodeLabel\n",
    "    RETURN nodeLabel, labels[nodeLabel] AS nodeCount\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total relationship counts\n",
    "gds.run_cypher(\"\"\"\n",
    "    CALL apoc.meta.stats()\n",
    "    YIELD relTypesCount\n",
    "    UNWIND keys(relTypesCount) AS relationshipType\n",
    "    RETURN relationshipType, relTypesCount[relationshipType] AS relationshipCount\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtJy4eO_zSqF"
   },
   "source": [
    "## Graph Data Science\n",
    "We got the data inside our Database! Let's do some Graph Data Science. This is how a typical GDS workflow looks like inside Neo4j\n",
    "\n",
    "![GDS Workflow](https://storage.googleapis.com/neo4j-datasets/insurance-claim/img/gds_workflow.png)\n",
    "\n",
    "As first step, we're going to use Neo4j Graph Data Science to create an in memory graph represtation of the data.  We'll enhance that representation with features we engineer using a graph embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 1320,
     "status": "ok",
     "timestamp": 1681109917108,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "gather": {
     "logged": 1669344835257
    },
    "id": "x76ZEtR16Oxb",
    "outputId": "84427470-4024-4c8e-b96d-0f889d2e5078"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "  \"\"\"\n",
    "    CALL gds.graph.project(\n",
    "      'projection',\n",
    "      ['Beneficiary','RenalDisease','IschemicHeartDisease','Osteoporosis',\n",
    "      'RheumatoidArthritis','Stroke','Diabetes','Depression','ObstructivePulmonaryDisease',\n",
    "      'Cancer','KidneyDisease','HeartFailure','Alzheimer','County','State','Claim',\n",
    "      'DiagnosisGroup','AdmitDiagnosis','Physician','Provider','Diagnosis','Procedure'],\n",
    "      {\n",
    "        LOCATED_AT: {orientation: 'UNDIRECTED'},\n",
    "        FILED_CLAIM: {orientation: 'UNDIRECTED'},\n",
    "        PROVIDED_BY: {orientation: 'UNDIRECTED'},\n",
    "        PART_OF: {orientation: 'UNDIRECTED'},\n",
    "        WORKS_FOR: {orientation: 'UNDIRECTED'},\n",
    "        ATTENDED_BY: {orientation: 'UNDIRECTED'},\n",
    "        OPERATED_BY: {orientation: 'UNDIRECTED'},\n",
    "        ALSO_ATTENDED_BY: {orientation: 'UNDIRECTED'},\n",
    "        HAS_ADMIT_DIAGNOSIS: {orientation: 'UNDIRECTED'},\n",
    "        HAS_DIAGNOSIS_GROUP: {orientation: 'UNDIRECTED'},\n",
    "        HAS_PROCEDURE: {orientation: 'UNDIRECTED'},\n",
    "        HAS_DIAGNOSIS: {orientation: 'UNDIRECTED'},\n",
    "        HAS_CHRONIC: {orientation: 'UNDIRECTED'},\n",
    "        HAS_DISEASE: {orientation: 'UNDIRECTED'}\n",
    "      }\n",
    "    )\n",
    "    YIELD\n",
    "      graphName AS graph,\n",
    "      relationshipProjection AS readProjection,\n",
    "      nodeCount AS nodes,\n",
    "      relationshipCount AS rels\n",
    "  \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiwL552u6Oxb"
   },
   "source": [
    "If you get an error saying the graph already exists, that's probably because you ran this code before. You can destroy it using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1681109905724,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "gather": {
     "logged": 1669263902233
    },
    "id": "EPZIIIJc6Oxb",
    "outputId": "a4b51e53-9b21-4f0f-fe6a-9a41fb4481cc"
   },
   "outputs": [],
   "source": [
    "# result = gds.run_cypher(\n",
    "#   \"\"\"\n",
    "#     CALL gds.graph.drop('projection')\n",
    "#   \"\"\"\n",
    "# )\n",
    "# display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zG1novOj6Oxb"
   },
   "source": [
    "Now, let's list the details of the graph to make sure the projection was created as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1681109923007,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "gather": {
     "logged": 1669344842885
    },
    "id": "yyaw5itE6Oxb",
    "outputId": "0e957f92-fe36-40d3-b658-fc8de5ffb369"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "  \"\"\"\n",
    "    CALL gds.graph.list()\n",
    "  \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEQAChAa6Oxb"
   },
   "source": [
    "Now we can generate an embedding from that graph. This is a new feature we can use in our predictions. We're using FastRP, which is a more full featured and higher performance of Node2Vec. You can learn more about that [here](https://neo4j.com/docs/graph-data-science/current/algorithms/fastrp/).\n",
    "\n",
    "There are a bunch of parameters we could adjust in this.  One of the most obvious is the embeddingDimension.  The documentation covers many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1681109936445,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "gather": {
     "logged": 1669344853127
    },
    "id": "qLFxuPb66Oxc",
    "outputId": "8ead5add-039b-4749-db21-ae1cb0595ae1"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "  \"\"\"\n",
    "  CALL gds.fastRP.mutate('projection',{\n",
    "    embeddingDimension: 32,\n",
    "    randomSeed: 1,\n",
    "    mutateProperty:'embedding'\n",
    "  })\n",
    "  \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRpgM-NV6Oxc"
   },
   "source": [
    "That creates an embedding for each node type.  However, we only want the embedding on the nodes of type holding.\n",
    "\n",
    "We're going to take the embedding from our projection and write it to the holding nodes in the underlying database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 1634,
     "status": "ok",
     "timestamp": 1681109940975,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "gather": {
     "logged": 1669344860116
    },
    "id": "3dBS16zD6Oxc",
    "outputId": "9a8be315-e872-4b79-99ea-25a0dacb7379",
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "  \"\"\"\n",
    "    CALL gds.graph.writeNodeProperties('projection', ['embedding'], \n",
    "    ['Claim'])\n",
    "    YIELD writeMillis\n",
    "  \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "  \"\"\" \n",
    "    MATCH (claim:Claim)\n",
    "    RETURN claim.id as id, claim.embedding as embedding, claim.isFraud as target\n",
    "    \n",
    "  \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we just did\n",
    "\n",
    "![embeddings](https://storage.googleapis.com/neo4j-datasets/insurance-claim/img/what_are_embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ5vwvnl8uLh"
   },
   "source": [
    "# Export Embeddings\n",
    "Now we're going to reformat the query output so that the embeddings can be fed in to a Vertex AI Auto ML pipeline. Note that we are exporting only embeddings and all the other features are intentionally left out. This is to showcase how powerful these vectors are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1681109970650,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "gather": {
     "logged": 1669344950284
    },
    "id": "197ZaAH16Oxc",
    "outputId": "4db4f980-74bf-4c2b-a304-3ea61ae834e0"
   },
   "outputs": [],
   "source": [
    "df = result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3esUO8s6Oxc"
   },
   "source": [
    "Note that the embedding row is an array. To make this dataset more consumable, we should flatten that out into multiple individual features: embedding_0, embedding_1, ... embedding_n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 811,
     "status": "ok",
     "timestamp": 1681109976089,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "gather": {
     "logged": 1669344969701
    },
    "id": "-i0_txCB6Oxc",
    "outputId": "77406878-d2d4-4960-8e62-5f4657cb08a0"
   },
   "outputs": [],
   "source": [
    "embeddings = pd.DataFrame(df['embedding'].values.tolist()).add_prefix(\"embedding_\")\n",
    "merged = df.drop(columns=['embedding']).merge(embeddings, left_index=True, right_index=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious, visualize the embeddings as a t-SNE plot. It can look something like this:\n",
    "![embedding_viz](https://storage.googleapis.com/neo4j-datasets/insurance-claim/img/embeddings-tsne.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Zb7lH366Oxc"
   },
   "source": [
    "Now that we have the data formatted properly, let's write it as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1711,
     "status": "ok",
     "timestamp": 1681109982494,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "gather": {
     "logged": 1669344975782
    },
    "id": "uLg34zlu6Oxc"
   },
   "outputs": [],
   "source": [
    "import os, numpy as np \n",
    "\n",
    "df = merged\n",
    "\n",
    "outdir = './data'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "data = df.sample(frac=1).reset_index(drop=True)\n",
    "data.to_csv(os.path.join(outdir, 'embedding.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Azb1inEVAC7f"
   },
   "source": [
    "## Upload to Google Cloud Storage\n",
    "Now let's write the file to Google Cloud Storage so we can use it in our model.  To do so, we must set a few environment variables.\n",
    "\n",
    "Edit the REGION variable below.  You'll want to be sure it matches the region where your notebook is running.\n",
    "\n",
    "The STORAGE_BUCKET is the name of a new bucket.  It must be globally unique.  It also needs to be all lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1681112445207,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "id": "xsDqLTwk8uLi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Edit this variable!\n",
    "REGION = 'us-west1'\n",
    "shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = shell_output[0]\n",
    "\n",
    "STORAGE_BUCKET = PROJECT_ID + '-fsi'\n",
    "STORAGE_BUCKET\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3103,
     "status": "ok",
     "timestamp": 1681112450308,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "id": "YHLPXZ198uLi"
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(STORAGE_BUCKET)\n",
    "if not bucket.exists:\n",
    "    bucket.create(location=REGION)\n",
    "\n",
    "blob = bucket.blob(os.path.join('insurance_fraud', 'embedding.csv'))\n",
    "blob.upload_from_filename(os.path.join(outdir, 'embedding.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also export factorize the raw data without embeddings and run a similar Auto ML pipeline on Vertex AI. Then, lets compare the accuracy between the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ew9mjD8-8uLi"
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"https://storage.googleapis.com/neo4j-datasets/insurance-claim/data.csv\")\n",
    "raw_df.rename(columns={'claim_id': 'id',  \n",
    "                       'is_fraud': 'target'}, inplace=True)\n",
    "raw_df['claim_diag_code_1'] = pd.factorize(raw_df['claim_diag_code_1'])[0] + 1\n",
    "raw_df['claim_diag_code_2'] = pd.factorize(raw_df['claim_diag_code_2'])[0] + 1\n",
    "raw_df['claim_diag_code_3'] = pd.factorize(raw_df['claim_diag_code_3'])[0] + 1\n",
    "raw_df['claim_diag_code_4'] = pd.factorize(raw_df['claim_diag_code_4'])[0] + 1\n",
    "raw_df['claim_diag_code_5'] = pd.factorize(raw_df['claim_diag_code_5'])[0] + 1\n",
    "raw_df['claim_diag_code_6'] = pd.factorize(raw_df['claim_diag_code_6'])[0] + 1\n",
    "raw_df['claim_diag_code_7'] = pd.factorize(raw_df['claim_diag_code_7'])[0] + 1\n",
    "raw_df['claim_diag_code_8'] = pd.factorize(raw_df['claim_diag_code_8'])[0] + 1\n",
    "raw_df['claim_diag_code_9'] = pd.factorize(raw_df['claim_diag_code_9'])[0] + 1\n",
    "raw_df['claim_diag_code_10'] = pd.factorize(raw_df['claim_diag_code_10'])[0] + 1\n",
    "raw_df['claim_procedure_code_1'] = pd.factorize(raw_df['claim_procedure_code_1'])[0] + 1\n",
    "raw_df['claim_procedure_code_2'] = pd.factorize(raw_df['claim_procedure_code_2'])[0] + 1\n",
    "raw_df['claim_procedure_code_3'] = pd.factorize(raw_df['claim_procedure_code_3'])[0] + 1\n",
    "raw_df['claim_procedure_code_4'] = pd.factorize(raw_df['claim_procedure_code_4'])[0] + 1\n",
    "raw_df['claim_procedure_code_5'] = pd.factorize(raw_df['claim_procedure_code_5'])[0] + 1\n",
    "raw_df['claim_procedure_code_6'] = pd.factorize(raw_df['claim_procedure_code_6'])[0] + 1\n",
    "raw_df['claim_admit_diagnosis_code'] = pd.factorize(raw_df['claim_admit_diagnosis_code'])[0] + 1\n",
    "raw_df['diag_group_code'] = pd.factorize(raw_df['diag_group_code'])[0] + 1\n",
    "\n",
    "raw_data = raw_df.sample(frac=1).reset_index(drop=True)\n",
    "raw_data.to_csv(os.path.join(outdir, 'raw.csv'), index=False)\n",
    "\n",
    "blob = bucket.blob(os.path.join('insurance_fraud', 'raw.csv'))\n",
    "blob.upload_from_filename(os.path.join(outdir, 'raw.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m106",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m106"
  },
  "instance_type": "ml.t3.medium",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
