{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rQ6G65n6OxV"
   },
   "source": [
    "# VertexAI Auto ML Embedding\n",
    "In this notebook, we'll use the awesome Vertex AI AutoML to train a model using our graph embedding as an additional feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9998,
     "status": "ok",
     "timestamp": 1681113488488,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "id": "zWcDg3rtFSTg"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet google-cloud-storage\n",
    "!pip install --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDT66GlIFSTh"
   },
   "source": [
    "## Vertex AI Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL79OSi1FSTh"
   },
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1681113149850,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "gather": {
     "logged": 1669345050388
    },
    "id": "7nS_fiKEFSTh"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaP3KG5bFSTh"
   },
   "source": [
    "### Workspace details\n",
    "Lets define the variables to connect to VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1681113268018,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "id": "ZBdYrgK4GHOH"
   },
   "outputs": [],
   "source": [
    "# Edit this variable!\n",
    "REGION = 'us-west1'\n",
    "shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = shell_output[0]\n",
    "\n",
    "STORAGE_BUCKET = PROJECT_ID + '-fsi'\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create Tabular dataset objects for our raw & embedding data below. These datasets refer to the Cloud Storage CSV files we just uploaded in the previous Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 471,
     "status": "error",
     "timestamp": 1681113462053,
     "user": {
      "displayName": "Ezhil Vendhan",
      "userId": "03023723423453260577"
     },
     "user_tz": -480
    },
    "gather": {
     "logged": 1669345052512
    },
    "id": "7SoC49BeFSTh",
    "outputId": "5a2cd7ea-4a27-4140-fdb9-7112bc5f83a6"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "raw_dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=\"claims-raw\",\n",
    "    gcs_source=os.path.join(\"gs://\", STORAGE_BUCKET, 'insurance_fraud', 'raw.csv'),\n",
    ")\n",
    "raw_dataset.wait()\n",
    "\n",
    "print(f'\\tDataset: \"{raw_dataset.display_name}\"')\n",
    "print(f'\\tname: \"{raw_dataset.resource_name}\"')\n",
    "\n",
    "embedding_dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=\"claims-embedding\",\n",
    "    gcs_source=os.path.join(\"gs://\", STORAGE_BUCKET, 'insurance_fraud', 'embedding.csv'),\n",
    ")\n",
    "embedding_dataset.wait()\n",
    "\n",
    "print(f'\\tDataset: \"{embedding_dataset.display_name}\"')\n",
    "print(f'\\tname: \"{embedding_dataset.resource_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets define the numeric columns in our RAW dataset and define the job that will help us classify fraudulent claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_column_names = [\n",
    "       'claim_amt_reimbursed', 'claim_diag_code_1', 'claim_diag_code_2',\n",
    "       'claim_diag_code_3', 'claim_diag_code_4', 'claim_diag_code_5',\n",
    "       'claim_diag_code_6', 'claim_diag_code_7', 'claim_diag_code_8',\n",
    "       'claim_diag_code_9', 'claim_diag_code_10', 'claim_procedure_code_1',\n",
    "       'claim_procedure_code_2', 'claim_procedure_code_3',\n",
    "       'claim_procedure_code_4', 'claim_procedure_code_5',\n",
    "       'claim_procedure_code_6', 'deductible_amt_paid',\n",
    "       'claim_admit_diagnosis_code', \n",
    "       'diag_group_code', 'gender', 'race',\n",
    "       'renal_disease_indicator', 'state', 'county',\n",
    "       'num_of_months_part_a_cov', 'num_of_months_part_b_Cov',\n",
    "       'chronic_cond_alzheimer', 'chronic_cond_heartfailure',\n",
    "       'chronic_cond_kidneydisease', 'chronic_cond_cancer',\n",
    "       'chronic_cond_obstrpulmonary', 'chronic_cond_depression',\n",
    "       'chronic_cond_diabetes', 'chronic_cond_ischemicheart',\n",
    "       'chronic_cond_osteoporasis', 'chronic_cond_rheumatoidarthritis',\n",
    "       'chronic_cond_stroke', 'ip_annual_reimbursement_amt',\n",
    "       'ip_annual_deductible_amt', 'op_annual_reimbursement_amt',\n",
    "       'op_annual_deductible_amt', 'target']\n",
    "column_specs = {column: \"numeric\" for column in num_column_names}\n",
    "\n",
    "raw_job = aiplatform.AutoMLTabularTrainingJob(\n",
    "    display_name=\"train-fraud-raw-automl\",\n",
    "    optimization_prediction_type=\"classification\",\n",
    "    column_specs=column_specs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, let's define the classifier job for the embedding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 32\n",
    "embedding_column_names = [\"embedding_{}\".format(i) for i in range(EMBEDDING_DIMENSION)]\n",
    "other_column_names = ['id']\n",
    "all_columns = embedding_column_names\n",
    "column_specs = {column: \"numeric\" for column in all_columns}\n",
    "\n",
    "embedding_job = aiplatform.AutoMLTabularTrainingJob(\n",
    "    display_name=\"train-fraud-embeddings-automl\",\n",
    "    optimization_prediction_type=\"classification\",\n",
    "    column_specs=column_specs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start running both the jobs now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model = raw_job.run(\n",
    "    dataset=raw_dataset,\n",
    "    target_column=\"target\",\n",
    "    training_fraction_split=0.8,\n",
    "    validation_fraction_split=0.1,\n",
    "    test_fraction_split=0.1,\n",
    "    model_display_name=\"insurance-fraud-prediction-model-raw\",\n",
    "    disable_early_stopping=False,\n",
    "    budget_milli_node_hours=1000,\n",
    "    sync = False\n",
    ")\n",
    "embedding_model = embedding_job.run(\n",
    "    dataset=embedding_dataset,\n",
    "    target_column=\"target\",\n",
    "    training_fraction_split=0.8,\n",
    "    validation_fraction_split=0.1,\n",
    "    test_fraction_split=0.1,\n",
    "    model_display_name=\"insurance-fraud-prediction-model-embedding\",\n",
    "    disable_early_stopping=False,\n",
    "    budget_milli_node_hours=1000,\n",
    "    sync = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 milli node hours, or one node hour, is the minimum budget that Vertex AI allows. However, Vertex AI isn't respecting that budget currently. This job will probably run for two and a half hours.\n",
    "\n",
    "We're going to move on while that runs. You can check on the job later in the Google Cloud Console to see the results. There's a link to the specific job in the output of the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Once completed, you can start comparing the training results of both of your models. My results look like below.\n",
    "I could see a ~10% improved F1 score and precision with my embedding model than the raw one.\n",
    "\n",
    "![Metrics](https://storage.googleapis.com/neo4j-datasets/insurance-claim/img/emb_vs_raw-metrics.png)\n",
    "\n",
    "![Metrics1](https://storage.googleapis.com/neo4j-datasets/insurance-claim/img/emb_vs_raw-metrics1.png)\n",
    "\n",
    "This the feature importance in both the models. As you can see embeddings could capture more meaningful features by relationships than the raw ones.\n",
    "\n",
    "![Feature Comparison](https://storage.googleapis.com/neo4j-datasets/insurance-claim/img/emb_vs_raw-features.png)\n",
    "\n",
    "Finally, this is the Confusion Matrix of the two models. The embedding algorithms has lesser false positives.\n",
    "\n",
    "![Confusion Matrix](https://storage.googleapis.com/neo4j-datasets/insurance-claim/img/emb_vs_raw-confusion.png)\n",
    "\n",
    "\n",
    "# Conclusion\n",
    "Vertex AI made our job simpler by taking care of lots of overheads like hyper parameter tuning, feature importance etc. Once you find your best model using Vertex AI, you can also export the features like embeddings generated using GDS to Vertex AI Feature Store, deploy your model endpoints and start doing some predictions. \n",
    "\n",
    "Neo4j GDS has more than 70 algorithms in the toolbox which can help you do Graph Data Science in a memory optimised platform. While we covered only FastRP embedding algorithm here, there are few more like GraphSAGE, Node2Vec, HashGNN etc. The models we tested out could be improved more and can include both raw and embedding features. We will leave it to you to try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m106",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m106"
  },
  "instance_type": "ml.t3.medium",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
